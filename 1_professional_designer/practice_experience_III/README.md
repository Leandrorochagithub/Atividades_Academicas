# PROJETO: Estudo de caso - √âtica e IA
### Defini√ß√£o do Problema
* Quais os limites da responsabilidade √©tica na Intelig√™ncia Artificial?

### Especifica√ß√µes T√©cnicas - M√©todo de An√°lise (framework)
* Vi√©s e Justi√ßa
* Transpar√™ncia e Explicabilidade
* Impacto Social e Direitos
* Responsabilidade e Governan√ßa
* Posicionamento
* **Relat√≥rio T√©cnico**

### Tecnologias utilizadas
* Mecaninsmos de busca na Web
* Word, Docs

### Aprendizado
* Import√¢ncia do planejamento e estudo do neg√≥cio
  
# √âtica e IA: Comparativo dos casos COMPAS e Amazon

**Um algoritmo pode ser injusto?** Analisamos o caso do sistema de recrutamento da Amazon e o do sistema de justi√ßa criminal (COMPAS) utilizado em alguns Estados dos EUA, que se revelaram enviesados contra determinados grupos da sociedade.

**O Problema**
As Intelig√™ncias Artificiais, treinadas com dados hist√≥ricos de sistemas preconceituosos, aprenderam a "perpetuar" a penaliza√ß√£o a indiv√≠duos baseando-se em seus g√™neros e suas ra√ßas. 

**Nossa An√°lise**
Os sistemas operavam como uma "black box", sem transpar√™ncia, com claro impacto social negativo.

**Nosso Posicionamento**
A decis√£o da Amazon de descontinuar o sistema foi correta. Recomendamos que futuras ferramentas de recrutamento com IA passem por auditorias de vi√©s rigorosas antes do lan√ßamento, e sejam supervisionadas por um comit√™ de √©tica multidisciplinar.
O Software COMPAS continua sendo usando em muitos casos, com as falhas sendo justificadas pela empresa desenvolvedora, o que revela a complexidade de lidar com esse tipo de ferramenta em institui√ß√µes p√∫blicas.

A inova√ß√£o n√£o pode ir de encontro √† equidade. Portanto, como podemos, enquanto profissionais de tecnologia, garantir que nossas cria√ß√µes sejam justas?

<img width="941" height="641" alt="Captura de tela 2025-09-18 200653" src="https://github.com/user-attachments/assets/496e56a7-82ff-4ea9-bbed-5706624cc66e" />

üëâ An√°lise Completa: No documento.
