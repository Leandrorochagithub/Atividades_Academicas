## √âtica e IA: Comparativo dos casos COMPAS e Amazon

**Um algoritmo pode ser injusto?** Analisamos o caso do sistema de recrutamento da Amazon e o do sistema de justi√ßa criminal (COMPAS) utilizado em alguns Estados dos EUA, que se revelaram enviesados contra determinados grupos da sociedade.

**O Problema**
As Intelig√™ncias Artificiais, treinadas com dados hist√≥ricos de sistemas preconceituosos, aprenderam a "perpetuar" a penaliza√ß√£o a indiv√≠duos baseando-se em seus g√™neros e suas ra√ßas. 

**Nossa An√°lise**
Os sistemas operavam como uma "black box", sem transpar√™ncia, com claro impacto social negativo.

**Nosso Posicionamento**
A decis√£o da Amazon de descontinuar o sistema foi correta. Recomendamos que futuras ferramentas de recrutamento com IA passem por auditorias de vi√©s rigorosas antes do lan√ßamento, e sejam supervisionadas por um comit√™ de √©tica multidisciplinar.
O Software COMPAS continua sendo usando em muitos casos, com as falhas sendo justificadas pela empresa desenvolvedora, o que revela a complexidade de lidar com esse tipo de ferramenta em institui√ß√µes p√∫blicas.

A inova√ß√£o n√£o pode ir de encontro √† equidade. Portanto, como podemos, enquanto profissionais de tecnologia, garantir que nossas cria√ß√µes sejam justas?

üëâ An√°lise Completa: No documento.
